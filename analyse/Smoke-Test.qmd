
---
title: "Traffic Monitoring System Analysis"
format: pdf
editor: visual
---

```{r setup, include=FALSE}
library(tidyverse)
library(vroom)
library(broom)

# Create output directories
if (!dir.exists("plots")) dir.create("plots")
```

# Data Loading and Preparation

```{r data-loading}
# Load all CSV files from logs directory
files <- list.files("logs/", pattern = "\\.csv$", full.names = TRUE)

df_raw <- vroom(files, id = "filepath", show_col_types = FALSE) %>%
  mutate(
    fps_target = as.numeric(str_extract(filepath, "\\d+(?=fps)")),
    run_id = basename(filepath)
  )

# Aggregate to device level (3 rows per run)
device_performance <- df_raw %>%
  group_by(run_id, source_device) %>%
  summarise(
    # Metadata
    mode           = first(mode),
    model_name     = first(model_name),
    codec          = first(codec),
    tier           = first(tier),
    resolution     = first(resolution),
    fps_target     = first(fps_target),
    
    # Latency metrics
    avg_latency_ms = mean(total_latency_us, na.rm = TRUE) / 1000,
    sd_latency_ms  = sd(total_latency_us, na.rm = TRUE) / 1000,
    inference_ms   = mean(inference_time_us, na.rm = TRUE) / 1000,
    overhead_ms    = avg_latency_ms - inference_ms,
    
    # Drop rate metrics
    actual_count   = n(),
    expected_count = first(fps_target) * 30,
    drop_rate_pct  = pmax(0, 1 - (actual_count / expected_count)) * 100,
    
    # Detection metrics
    avg_detections = mean(detection_count, na.rm = TRUE),
    
    .groups = "drop"
  )
```

# Statistical Analysis

```{r statistical-analysis}
# Mode comparison (Local vs Offload)
mode_comparison <- device_performance %>%
  group_by(model_name, codec, tier, resolution, fps_target, mode) %>%
  summarise(
    mean_lat = mean(avg_latency_ms, na.rm = TRUE),
    mean_drop = mean(drop_rate_pct, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = mode, 
    values_from = c(mean_lat, mean_drop)
  ) %>%
  mutate(
    latency_improvement = mean_lat_Local - mean_lat_Offload,
    drop_reduction = mean_drop_Local - mean_drop_Offload
  )

# Linear model to assess factors (latency)
model_latency <- lm(avg_latency_ms ~ mode + resolution + codec + source_device + fps_target, 
                    data = device_performance)

summary(model_latency)

# Verify device counts per run
device_performance %>% 
  count(run_id) %>% 
  head()
```

# Drop Rate Regression Analysis (Grouped by FPS)

```{r drop-rate-regression}
# Run separate regressions for each FPS target
fps_list <- sort(unique(device_performance$fps_target))

drop_rate_models <- list()
drop_rate_summaries <- list()

for (current_fps in fps_list) {
  cat("\n===========================================\n")
  cat("FPS TARGET:", current_fps, "\n")
  cat("===========================================\n\n")
  
  # Filter data for this FPS
  fps_data <- device_performance %>%
    filter(fps_target == current_fps)
  
  # Fit model with model_name as key predictor
  model <- lm(drop_rate_pct ~ model_name + mode + resolution + codec + tier + source_device,
              data = fps_data)
  
  # Store model
  drop_rate_models[[paste0("fps_", current_fps)]] <- model
  
  # Print summary
  print(summary(model))
  
  # Extract model coefficients for comparison
  coef_summary <- broom::tidy(model) %>%
    filter(str_detect(term, "^model_name")) %>%
    arrange(estimate)
  
  drop_rate_summaries[[paste0("fps_", current_fps)]] <- coef_summary
  
  cat("\n--- Model Performance Rankings ---\n")
  print(coef_summary)
  cat("\n")
}

# Combine all summaries for visualization
all_model_effects <- bind_rows(drop_rate_summaries, .id = "fps_config") %>%
  mutate(
    fps = as.numeric(str_extract(fps_config, "\\d+")),
    model = str_remove(term, "^model_name")
  )

# Plot model effects across FPS targets
ggplot(all_model_effects, aes(x = model, y = estimate, fill = as.factor(fps))) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7) +
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error),
                position = position_dodge(width = 0.8), width = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Model Effect on Drop Rate by FPS Target",
    subtitle = "Negative values = lower drop rate (better). Reference model excluded.",
    x = "Model",
    y = "Effect on Drop Rate (%)",
    fill = "FPS Target"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Executive Summary

The linear regression analysis reveals **clear performance differences between YOLO models** across all FPS targets, with YOLOv5n consistently outperforming both YOLOv5s and the baseline (likely YOLOv5m or YOLOv8m based on the intercept values).

### Key Findings:

- **YOLOv5n is the best performer** - reduces drop rates by 14-48% compared to baseline
- **Offload mode significantly improves performance** - reduces drops by 3-10%
- **Performance advantage decreases at higher FPS** - suggesting YOLOv5n hits processing limits
- **Model choice matters more than hardware variance** - source device has minimal impact

------------------------------------------------------------------------

## Analysis by FPS Target

### 1 FPS (Low Frame Rate)

**Model Performance:** - **Baseline drop rate**: 51.6% - **YOLOv5n**: -48.1% (effective drop rate: \~3.5%) - **YOLOv5s**: -46.6% (effective drop rate: \~5.0%)

**Key Insights:** - At 1 FPS, both lightweight models perform exceptionally well - The baseline model struggles significantly (\>50% drops) - YOLOv5n has a slight edge (1.5% better than YOLOv5s) - **Offload mode helps significantly** (-5.8% drops, p \< 0.001) - Letterbox resolution provides better results than native (-2.4% drops)

**Statistical Strength:** - R² = 0.958 (model explains 95.8% of variance) - All model effects highly significant (p \< 0.001)

------------------------------------------------------------------------

### 5 FPS (Medium-Low Frame Rate)

**Model Performance:** - **Baseline drop rate**: 95.6% (nearly all frames dropped!) - **YOLOv5n**: -44.1% (effective drop rate: \~51.5%) - **YOLOv5s**: -13.3% (effective drop rate: \~82.3%)

**Key Insights:** - **Critical finding**: YOLOv5n maintains viability while YOLOv5s deteriorates - Gap between YOLOv5n and YOLOv5s widens dramatically (30.8% difference) - Baseline model is completely overwhelmed at this frame rate - **Offload mode becomes more critical** (-9.5% drops) - Letterbox resolution shows major benefit (-8.0% drops)

**Statistical Strength:** - R² = 0.890 (still strong explanatory power) - Higher residuals indicate more variability at this FPS

**Warning Signs:** - Codec choice now matters (PNG/WebP add \~3% drops each) - T3 tier quality negatively impacts performance (+1.3% drops)

------------------------------------------------------------------------

### 10 FPS (Medium-High Frame Rate)

**Model Performance:** - **Baseline drop rate**: 97.7% (system nearly saturated) - **YOLOv5n**: -21.6% (effective drop rate: \~76.1%) - **YOLOv5s**: -6.6% (effective drop rate: \~91.1%)

**Key Insights:** - **YOLOv5n's advantage decreases** from 30.8% to 15% over YOLOv5s - Both models are struggling; drops exceed 75% even for best model - **Offload mode still helps** (-4.5% drops) but effect is diminishing - Letterbox resolution benefit reduces to -3.5%

**Statistical Strength:** - R² = 0.902 (excellent fit) - Lower residual error (3.15) suggests more consistent behavior

**Codec Impact Grows:** - WebP Lossy worst offender (+1.6% drops) - PNG Lossless adds +1.0% drops

------------------------------------------------------------------------

### 15 FPS (High Frame Rate)

**Model Performance:** - **Baseline drop rate**: 98.6% (complete saturation) - **YOLOv5n**: -14.5% (effective drop rate: \~84.1%) - **YOLOv5s**: -4.5% (effective drop rate: \~94.1%)

**Key Insights:** - **All models saturated** - even YOLOv5n dropping \>80% of frames - YOLOv5n advantage shrinks to 10% over YOLOv5s - **Offload mode benefit reduces** to -3.2% (less effective at saturation) - System is fundamentally bandwidth/compute limited

**Statistical Strength:** - R² = 0.904 (consistent fit) - Lowest residual error (2.08) - system behaving predictably at limits

**Resolution Impact Minimal:** - HD: -0.6% drops - Letterbox: -1.7% drops - Differences are negligible when system is saturated

------------------------------------------------------------------------

## Cross-FPS Trends

### Model Performance Degradation

```         
YOLOv5n benefit vs baseline:
1 FPS:  -48.1% (excellent)
5 FPS:  -44.1% (very good)
10 FPS: -21.6% (moderate)
15 FPS: -14.5% (limited)
```

**Interpretation**: YOLOv5n's advantage erodes as FPS increases, suggesting it reaches computational limits around 5-10 FPS.

### Offload Mode Effectiveness

```         
Offload benefit:
1 FPS:  -5.8% (strong)
5 FPS:  -9.5% (peak benefit)
10 FPS: -4.5% (moderate)
15 FPS: -3.2% (weak)
```

**Interpretation**: Offload mode is most effective at 5 FPS, suggesting optimal balance between network overhead and computational offloading. At higher FPS, network bandwidth becomes the bottleneck. Note on Network Bandwidth, if a Frame is waiting to be processed that also "Network"

### Codec/Quality Tier Impact

- **1 FPS**: Minimal impact (system has headroom)
- **5+ FPS**: PNG/WebP add 1-3.7% drops
- **Higher tiers**: Generally neutral, occasionally harmful

**Interpretation**: Compression overhead becomes significant only when system is under stress.

------------------------------------------------------------------------

## Statistical Validity

### Model Fit Quality (R²)

- 1 FPS: 0.958 (excellent)
- 5 FPS: 0.890 (very good)
- 10 FPS: 0.902 (excellent)
- 15 FPS: 0.904 (excellent)

All models explain \>89% of variance, indicating strong predictive power.

### Coefficient Significance

- **Model name**: p \< 0.001 across all FPS (highly significant)
- **Mode (Offload)**: p \< 0.001 across all FPS (highly significant)
- **Source device**: p \> 0.05 (NOT significant - hardware variance negligible)
- **Tier**: p \> 0.05 (NOT significant in most cases)

------------------------------------------------------------------------

## Recommendations

### 1. Model Selection

- **For low FPS (1-5)**: Use YOLOv5n in offload mode
  - Expected drop rate: 3-50%
  - Letterbox resolution preferred
- **For medium FPS (10)**: Use YOLOv5n with realistic expectations
  - Expected drop rate: 75%+
  - Consider reducing FPS target or increasing hardware
- **For high FPS (15+)**: System redesign required
  - Current setup cannot support 15 FPS reliably
  - Consider: multiple GPUs, better network, or accept high drop rates

### 2. Configuration Optimization

- **Resolution**: Use Letterbox when bandwidth constrained
- **Codec**: Avoid PNG/WebP at high FPS (use JPEG or reduce quality)
- **Quality Tier**: T1 (lowest) recommended - higher tiers don't improve detection but add overhead

### 3. Offload Strategy

- **Enable offload** for all configurations
- Peak benefit at 5 FPS, but helps across the board
- At 15 FPS, diminishing returns suggest network is saturated

### 4. Hardware Scaling

- **Source device variance is negligible** - don't waste resources on device upgrades
- Focus on: network bandwidth, GPU acceleration, or reduced FPS targets

------------------------------------------------------------------------

## Limitations & Considerations

1.  **Baseline model unknown**: Analysis assumes third model (YOLOv8m or YOLOv5m) as reference
2.  **Linear assumptions**: Saturation effects may not be perfectly linear
3.  **Interaction effects**: Model performance may interact with resolution/codec differently than captured
4.  **30-second duration**: Longer runs might show different thermal/stability characteristics

------------------------------------------------------------------------

## Conclusion

**YOLOv5n is definitively the best-performing model** across all tested frame rates, but its advantage diminishes as system saturation increases. The analysis demonstrates that:

1.  **Model architecture matters more than hardware** (RSU device variance is insignificant)
2.  **Offload mode provides consistent benefit** but cannot overcome fundamental bandwidth limits
3.  **System is not viable above 10 FPS** with current hardware/configuration
4.  **Focus optimization efforts on**: model selection, offload enablement, and realistic FPS targets

The regression models are highly reliable (R² \> 0.89) and statistically significant, providing strong confidence in these conclusions.

# Overall Drop Rate Model (All FPS Combined)

```{r overall-drop-model}
# Single model across all FPS with interaction terms
overall_model <- lm(drop_rate_pct ~ model_name * fps_target + mode + resolution + codec + tier + source_device,
                    data = device_performance)

cat("\n===========================================\n")
cat("OVERALL DROP RATE MODEL (ALL FPS)\n")
cat("===========================================\n\n")
print(summary(overall_model))

# Extract model-specific effects
model_effects <- broom::tidy(overall_model) %>%
  filter(str_detect(term, "^model_name") & !str_detect(term, ":")) %>%
  arrange(estimate)

cat("\n--- Overall Model Rankings (Main Effects) ---\n")
print(model_effects)

# ANOVA to test if model_name is significant predictor
cat("\n--- ANOVA: Model Name Significance ---\n")
print(anova(overall_model))

# Estimated marginal means for each model
cat("\n--- Predicted Drop Rates by Model (Averaged Across Conditions) ---\n")
model_predictions <- device_performance %>%
  group_by(model_name) %>%
  summarise(
    mean_drop = mean(drop_rate_pct, na.rm = TRUE),
    sd_drop = sd(drop_rate_pct, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) %>%
  arrange(mean_drop)

print(model_predictions)

# Visualization
ggplot(model_predictions, aes(x = reorder(model_name, mean_drop), y = mean_drop)) +
  geom_col(fill = "#3498db", alpha = 0.8) +
  geom_errorbar(aes(ymin = mean_drop - sd_drop, ymax = mean_drop + sd_drop), 
                width = 0.3, alpha = 0.6) +
  labs(
    title = "Average Drop Rate by Model (All Conditions)",
    subtitle = "Error bars show ± 1 SD",
    x = "Model",
    y = "Mean Drop Rate (%)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Performance Comparison by FPS

```{r performance-comparison, fig.width=10, fig.height=6}
fps_list <- sort(unique(device_performance$fps_target))

for (current_fps in fps_list) {
  p <- device_performance %>%
    filter(fps_target == current_fps) %>%
    ggplot(aes(x = model_name, y = avg_latency_ms, fill = mode)) +
    geom_boxplot(alpha = 0.7, outlier.shape = NA) +
    geom_jitter(position = position_jitterdodge(jitter.width = 0.1), size = 1, alpha = 0.5) +
    facet_wrap(~resolution, scales = "free_y") +
    labs(
      title = paste("Performance Comparison at", current_fps, "FPS"),
      subtitle = "Comparing Local vs. Offload execution across models",
      x = "Model Name",
      y = "Latency (ms)",
      fill = "Mode"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
}
```

# Latency Breakdown (Inference vs Overhead)

```{r latency-breakdown, fig.width=10, fig.height=6}
for (current_fps in fps_list) {
  p <- device_performance %>%
    filter(fps_target == current_fps) %>%
    pivot_longer(cols = c(inference_ms, overhead_ms),
                 names_to = "component", values_to = "ms") %>%
    ggplot(aes(x = mode, y = ms, fill = component)) +
    geom_bar(stat = "summary", fun = "mean", position = "stack", width = 0.7) +
    facet_grid(resolution ~ model_name) +
    scale_fill_manual(
      values = c("inference_ms" = "#27ae60", "overhead_ms" = "#e74c3c"),
      labels = c("Inference", "Network/Transport Overhead")
    ) +
    labs(
      title = paste("Latency Breakdown at", current_fps, "FPS"),
      subtitle = "Green = AI Work | Red = Network Tax",
      x = "Execution Mode",
      y = "Total End-to-End Latency (ms)",
      fill = "Component"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  print(p)
}
```

# Comprehensive Analysis (All Metrics)

```{r comprehensive-analysis, fig.width=10, fig.height=6}
all_fps    <- sort(unique(device_performance$fps_target))
all_codecs <- unique(device_performance$codec)
all_tiers  <- sort(unique(device_performance$tier))

for (f in all_fps) {
  for (c in all_codecs) {
    for (t in all_tiers) {
      plot_subset <- device_performance %>%
        filter(fps_target == f, codec == c, tier == t)
      
      if(nrow(plot_subset) == 0) next
      
      # Latency plot
      p_lat <- ggplot(plot_subset, aes(x = model_name, y = avg_latency_ms, fill = mode)) +
        geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
        geom_jitter(position = position_dodge(width = 0.8), size = 1.2, alpha = 0.5) +
        facet_wrap(~resolution, scales = "free_y") +
        scale_fill_manual(values = c("Local" = "#2c3e50", "Offload" = "#27ae60")) +
        labs(title = paste("Latency |", f, "FPS |", c, "|", t), 
             y = "Avg Latency (ms)", x = "Model") +
        theme_minimal()
      
      # Drop rate plot
      p_drop <- ggplot(plot_subset, aes(x = model_name, y = drop_rate_pct, fill = mode)) +
        geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
        geom_jitter(position = position_dodge(width = 0.8), size = 1, alpha = 0.4) +
        facet_wrap(~resolution) +
        scale_fill_manual(values = c("Local" = "#34495e", "Offload" = "#d35400")) +
        scale_y_continuous(limits = c(0, 100)) +
        labs(title = paste("Drop Rate |", f, "FPS |", c, "|", t), 
             y = "Drop Rate (%)", x = "Model") +
        theme_minimal()
      
      # Detection plot
      p_det <- ggplot(plot_subset, aes(x = model_name, y = avg_detections, fill = mode)) +
        geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
        geom_jitter(position = position_dodge(width = 0.8), size = 1.2, alpha = 0.5) +
        facet_wrap(~resolution) +
        scale_fill_manual(values = c("Local" = "#2c3e50", "Offload" = "#8e44ad")) +
        labs(
          title = paste("Avg Detections |", f, "FPS"),
          subtitle = paste("Codec:", c, "| Quality Tier:", t),
          x = "AI Model",
          y = "Avg Detection Count per Frame",
          fill = "Execution Mode"
        ) +
        theme_minimal() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
      
      # Print plots
      print(p_lat)
      print(p_drop)
      print(p_det)
      
      # Save plots
      file_base <- paste0("fps", f, "_", c, "_", t)
      ggsave(paste0("plots/latency_", file_base, ".png"), p_lat, width = 10, height = 6)
      ggsave(paste0("plots/drops_", file_base, ".png"), p_drop, width = 10, height = 6)
      ggsave(paste0("plots/det_", file_base, ".png"), p_det, width = 10, height = 6)
      
      message("Saved plots for: ", file_base)
    }
  }
}
```
