mutate(
fps_target = as.numeric(str_extract(filepath, "\\d+(?=fps)")),
run_id = basename(filepath)
)
# 2. Aggregate to Device Level (3 rows per run)
# ---------------------------------------------------------
device_perf <- df_raw %>%
group_by(run_id, source_device) %>%
summarise(
mode           = first(mode),
model_name     = first(model_name),
resolution     = first(resolution),
fps_target     = first(fps_target),
avg_latency_ms = mean(total_latency_us, na.rm = TRUE) / 1000,
.groups = "drop"
)
# 3. Generate Individual Graphs per FPS Target
# ---------------------------------------------------------
# Get list of unique FPS targets (e.g., 1, 5, 10, 30)
fps_list <- sort(unique(device_perf$fps_target))
# Loop through each FPS target and create a separate plot
for (current_fps in fps_list) {
plot_data <- device_perf %>% filter(fps_target == current_fps)
p <- ggplot(plot_data, aes(x = model_name, y = avg_latency_ms, fill = mode)) +
# Use boxplots to show the distribution across the 3 devices
geom_boxplot(alpha = 0.7, outlier.shape = NA) +
# Add dots for the 3 individual devices to show exact variance
geom_jitter(position = position_jitterdodge(jitter.width = 0.1), size = 1, alpha = 0.5) +
# Separate by resolution so the Y-axis scales don't squash the data
facet_wrap(~resolution, scales = "free_y") +
labs(
title = paste("Performance Comparison at", current_fps, "FPS"),
subtitle = "Comparing Local vs. Offload execution across models",
x = "Model Name",
y = "Latency (ms)",
fill = "Mode"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Display the plot in your R session
print(p)
# Optional: Save each one as a file automatically
# ggsave(paste0("plot_fps_", current_fps, ".png"), p, width = 10, height = 6)
}
library(tidyverse)
# 1. Aggregate and Calculate Overheads
# ---------------------------------------------------------
device_perf <- df_raw %>%
group_by(run_id, source_device) %>%
summarise(
mode           = first(mode),
model_name     = first(model_name),
resolution     = first(resolution),
fps_target     = first(fps_target),
# Inference vs Everything Else
# We convert us to ms
inference_ms   = mean(inference_time_us, na.rm = TRUE) / 1000,
total_ms       = mean(total_latency_us, na.rm = TRUE) / 1000,
network_ovh_ms = total_ms - inference_ms,
.groups = "drop"
)
# 2. Loop: One Graph per FPS
# ---------------------------------------------------------
fps_list <- sort(unique(device_perf$fps_target))
for (current_fps in fps_list) {
# Prepare data for stacked bar chart to see "Where the time goes"
plot_data <- device_perf %>%
filter(fps_target == current_fps) %>%
select(model_name, mode, resolution, inference_ms, network_ovh_ms) %>%
pivot_longer(cols = c(inference_ms, network_ovh_ms),
names_to = "component", values_to = "ms")
p <- ggplot(plot_data, aes(x = mode, y = ms, fill = component)) +
geom_bar(stat = "summary", fun = "mean", position = "stack") +
facet_grid(resolution ~ model_name) +
scale_fill_manual(values = c("inference_ms" = "#2c3e50", "network_ovh_ms" = "#e74c3c"),
labels = c("Inference (Compute)", "Overhead (Network/Enc)")) +
labs(
title = paste("Latency Breakdown at", current_fps, "FPS"),
subtitle = "Why Offloading might be slower (Red = Network/Transport Overhead)",
x = "Execution Mode",
y = "Total Latency (ms)",
fill = "Component"
) +
theme_minimal()
print(p)
}
library(tidyverse)
# 1. Create the detailed Device-Level object
# ---------------------------------------------------------
device_perf <- df_raw %>%
group_by(run_id, source_device) %>%
summarise(
mode           = first(mode),
model_name     = first(model_name),
resolution     = first(resolution),
fps_target     = first(fps_target),
# Calculate components in milliseconds
inference_ms   = mean(inference_time_us, na.rm = TRUE) / 1000,
total_ms       = mean(total_latency_us, na.rm = TRUE) / 1000,
# "Overhead" is everything except the AI compute (Network + Encode + Serialization)
overhead_ms    = total_ms - inference_ms,
.groups = "drop"
)
# 2. Loop: Generate one graph per FPS target
# ---------------------------------------------------------
fps_list <- sort(unique(device_perf$fps_target))
for (current_fps in fps_list) {
# Pivot longer so we can stack Inference and Overhead in the same bar
plot_data <- device_perf %>%
filter(fps_target == current_fps) %>%
pivot_longer(cols = c(inference_ms, overhead_ms),
names_to = "component", values_to = "ms")
p <- ggplot(plot_data, aes(x = mode, y = ms, fill = component)) +
# Sum the averages to show total latency breakdown
geom_bar(stat = "summary", fun = "mean", position = "stack", width = 0.7) +
# Separate by Resolution and Model for total clarity
facet_grid(resolution ~ model_name) +
scale_fill_manual(
values = c("inference_ms" = "#27ae60", "overhead_ms" = "#e74c3c"),
labels = c("Inference (The 100ms vs 500ms part)", "Network/Transport Overhead")
) +
labs(
title = paste("LATENCY BREAKDOWN AT", current_fps, "FPS"),
subtitle = "Green is AI Work | Red is the 'Network Tax'",
x = "Execution Mode",
y = "Total End-to-End Latency (ms)",
fill = "Where is the time going?"
) +
theme_minimal() +
theme(legend.position = "bottom")
print(p)
}
library(tidyverse)
library(vroom)
# 1. Load and Pre-process Data
# ---------------------------------------------------------
files <- list.files("logs/", pattern = "\\.csv$", full.names = TRUE)
df_raw <- vroom(files, id = "filepath", show_col_types = FALSE) %>%
mutate(
# Extract FPS from filename (e.g., "log_10fps.csv" -> 10)
fps_target = as.numeric(str_extract(filepath, "\\d+(?=fps)")),
run_id = basename(filepath)
)
# 2. Aggregate to Device Level (3 rows per CSV)
# ---------------------------------------------------------
drop_data_detailed <- df_raw %>%
group_by(run_id, source_device) %>%
summarise(
mode           = first(mode),
model_name     = first(model_name),
resolution     = first(resolution),
fps_target     = first(fps_target),
codec          = first(codec),
tier           = first(tier), # Quality Tier (T1, T2, etc.)
# Drop Rate Calculation (30 second duration)
actual_count   = n(),
expected_count = first(fps_target) * 30,
drop_rate_pct  = pmax(0, 1 - (actual_count / expected_count)) * 100,
.groups = "drop"
)
# 3. Triple-Nested Loop: FPS x Codec x Tier
# ---------------------------------------------------------
all_fps    <- sort(unique(drop_data_detailed$fps_target))
all_codecs <- unique(drop_data_detailed$codec)
all_tiers  <- sort(unique(drop_data_detailed$tier))
for (f in all_fps) {
for (c in all_codecs) {
for (t in all_tiers) {
# Filter for this specific scenario
plot_subset <- drop_data_detailed %>%
filter(fps_target == f, codec == c, tier == t)
# Only generate the plot if data exists for this combination
if(nrow(plot_subset) == 0) next
p <- ggplot(plot_subset, aes(x = model_name, y = drop_rate_pct, fill = mode)) +
# Show average drop rate for the 3 devices
geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
# Add points to see individual device performance
geom_jitter(position = position_dodge(width = 0.8), size = 1, alpha = 0.4) +
# Facet by resolution to see how data volume affects things
facet_wrap(~resolution) +
scale_fill_manual(values = c("Local" = "#34495e", "Offload" = "#d35400")) +
scale_y_continuous(limits = c(0, 100)) + # Keep scale consistent
labs(
title = paste(f, "FPS | Codec:", c, "| Tier:", t),
subtitle = "Comparing Frame Drop Rates: Lower is Better",
x = "Model Name",
y = "Drop Rate (%)",
fill = "Execution Mode"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid.minor = element_blank()
)
# Output plot to the R plot window
print(p)
# Optional: Uncomment to save each plot automatically
# ggsave(paste0("drop_fps", f, "_", c, "_", t, ".png"), p, width = 8, height = 5)
}
}
}
library(tidyverse)
library(vroom)
# 1. Load and Pre-process Data
# ---------------------------------------------------------
files <- list.files("logs/", pattern = "\\.csv$", full.names = TRUE)
df_raw <- vroom(files, id = "filepath", show_col_types = FALSE) %>%
mutate(
# Extract FPS from filename (e.g., "log_10fps.csv" -> 10)
fps_target = as.numeric(str_extract(filepath, "\\d+(?=fps)")),
run_id = basename(filepath)
)
# 2. Aggregate to Device Level (3 rows per CSV)
# ---------------------------------------------------------
drop_data_detailed <- df_raw %>%
group_by(run_id, source_device) %>%
summarise(
mode           = first(mode),
model_name     = first(model_name),
resolution     = first(resolution),
fps_target     = first(fps_target),
codec          = first(codec),
tier           = first(tier), # Quality Tier (T1, T2, etc.)
# Drop Rate Calculation (30 second duration)
actual_count   = n(),
expected_count = first(fps_target) * 30,
drop_rate_pct  = pmax(0, 1 - (actual_count / expected_count)) * 100,
.groups = "drop"
)
# 3. Triple-Nested Loop: FPS x Codec x Tier
# ---------------------------------------------------------
all_fps    <- sort(unique(drop_data_detailed$fps_target))
all_codecs <- unique(drop_data_detailed$codec)
all_tiers  <- sort(unique(drop_data_detailed$tier))
for (f in all_fps) {
for (c in all_codecs) {
for (t in all_tiers) {
# Filter for this specific scenario
plot_subset <- drop_data_detailed %>%
filter(fps_target == f, codec == c, tier == t)
# Only generate the plot if data exists for this combination
if(nrow(plot_subset) == 0) next
p <- ggplot(plot_subset, aes(x = model_name, y = drop_rate_pct, fill = mode)) +
# Show average drop rate for the 3 devices
geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
# Add points to see individual device performance
geom_jitter(position = position_dodge(width = 0.8), size = 1, alpha = 0.4) +
# Facet by resolution to see how data volume affects things
facet_wrap(~resolution) +
scale_fill_manual(values = c("Local" = "#34495e", "Offload" = "#d35400")) +
scale_y_continuous(limits = c(0, 100)) + # Keep scale consistent
labs(
title = paste(f, "FPS | Codec:", c, "| Tier:", t),
subtitle = "Comparing Frame Drop Rates: Lower is Better",
x = "Model Name",
y = "Drop Rate (%)",
fill = "Execution Mode"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid.minor = element_blank()
)
# Output plot to the R plot window
print(p)
# Optional: Uncomment to save each plot automatically
ggsave(paste0("drop_fps", f, "_", c, "_", t, ".png"), p, width = 8, height = 5)
}
}
}
library(tidyverse)
library(vroom)
# 1. Load and Aggregation
# ---------------------------------------------------------
files <- list.files("logs/", pattern = "\\.csv$", full.names = TRUE)
df_latency <- vroom(files, id = "filepath", show_col_types = FALSE) %>%
mutate(
fps_target = as.numeric(str_extract(filepath, "\\d+(?=fps)")),
run_id = basename(filepath)
) %>%
group_by(run_id, source_device) %>%
summarise(
mode           = first(mode),
model_name     = first(model_name),
resolution     = first(resolution),
fps_target     = first(fps_target),
codec          = first(codec),
tier           = first(tier),
# Mean total latency in milliseconds
avg_latency_ms = mean(total_latency_us, na.rm = TRUE) / 1000,
.groups = "drop"
)
# 2. Triple-Nested Loop: FPS x Codec x Tier
# ---------------------------------------------------------
all_fps    <- sort(unique(df_latency$fps_target))
all_codecs <- unique(df_latency$codec)
all_tiers  <- sort(unique(df_latency$tier))
for (f in all_fps) {
for (c in all_codecs) {
for (t in all_tiers) {
# Filter for this specific scenario
plot_subset <- df_latency %>%
filter(fps_target == f, codec == c, tier == t)
# Only generate if data exists
if(nrow(plot_subset) == 0) next
p <- ggplot(plot_subset, aes(x = model_name, y = avg_latency_ms, fill = mode)) +
# Compare Local vs Offload side-by-side
geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
# Add the 3 device points to check for outliers/jitter
geom_jitter(position = position_dodge(width = 0.8), size = 1.2, alpha = 0.5) +
facet_wrap(~resolution, scales = "free_y") +
scale_fill_manual(values = c("Local" = "#2c3e50", "Offload" = "#27ae60")) +
labs(
title = paste("End-to-End Latency at", f, "FPS"),
subtitle = paste("Codec:", c, "| Quality Tier:", t),
x = "AI Model",
y = "Average Latency (ms)",
fill = "Execution Mode"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(p)
}
}
}
library(tidyverse)
library(vroom)
# 1. Load and Aggregation
# ---------------------------------------------------------
# Ensure you are in the correct working directory where your 'logs/' folder exists
files <- list.files("logs/", pattern = "\\.csv$", full.names = TRUE)
df_analysis <- vroom(files, id = "filepath", show_col_types = FALSE) %>%
mutate(
fps_target = as.numeric(str_extract(filepath, "\\d+(?=fps)")),
run_id = basename(filepath)
) %>%
group_by(run_id, source_device) %>%
summarise(
mode           = first(mode),
model_name     = first(model_name),
resolution     = first(resolution),
fps_target     = first(fps_target),
codec          = first(codec),
tier           = first(tier),
# Metrics
avg_latency_ms = mean(total_latency_us, na.rm = TRUE) / 1000,
drop_rate_pct  = pmax(0, 1 - (n() / (first(fps_target) * 30))) * 100,
.groups = "drop"
)
# Create a directory to store the plots
if (!dir.exists("plots")) dir.create("plots")
# 2. Triple-Nested Loop for Latency and Drops
# ---------------------------------------------------------
all_fps    <- sort(unique(df_analysis$fps_target))
all_codecs <- unique(df_analysis$codec)
all_tiers  <- sort(unique(df_analysis$tier))
for (f in all_fps) {
for (c in all_codecs) {
for (t in all_tiers) {
# Filter for this specific scenario
plot_subset <- df_analysis %>%
filter(fps_target == f, codec == c, tier == t)
if(nrow(plot_subset) == 0) next
# --- GENERATE LATENCY PLOT ---
p_lat <- ggplot(plot_subset, aes(x = model_name, y = avg_latency_ms, fill = mode)) +
geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
geom_jitter(position = position_dodge(width = 0.8), size = 1.2, alpha = 0.5) +
facet_wrap(~resolution, scales = "free_y") +
scale_fill_manual(values = c("Local" = "#2c3e50", "Offload" = "#27ae60")) +
labs(title = paste("Latency |", f, "FPS |", c, "|", t), y = "Avg Latency (ms)", x = "Model") +
theme_minimal()
# --- GENERATE DROP RATE PLOT ---
p_drop <- ggplot(plot_subset, aes(x = model_name, y = drop_rate_pct, fill = mode)) +
geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
facet_wrap(~resolution) +
scale_fill_manual(values = c("Local" = "#34495e", "Offload" = "#d35400")) +
scale_y_continuous(limits = c(0, 100)) +
labs(title = paste("Drop Rate |", f, "FPS |", c, "|", t), y = "Drop Rate (%)", x = "Model") +
theme_minimal()
# 3. SAVE RESULTS
# ---------------------------------------------------------
# Filenames use variables to prevent overwriting
file_base <- paste0("fps", f, "_", c, "_", t)
ggsave(filename = paste0("plots/latency_", file_base, ".png"), plot = p_lat, width = 10, height = 6)
ggsave(filename = paste0("plots/drops_", file_base, ".png"), plot = p_drop, width = 10, height = 6)
# Optional: Print to console to track progress
message("Saved plots for: ", file_base)
}
}
}
library(tidyverse)
library(vroom)
# 1. Load and Aggregation
# ---------------------------------------------------------
files <- list.files("logs/", pattern = "\\.csv$", full.names = TRUE)
df_detections <- vroom(files, id = "filepath", show_col_types = FALSE) %>%
mutate(
fps_target = as.numeric(str_extract(filepath, "\\d+(?=fps)")),
run_id = basename(filepath)
) %>%
group_by(run_id, source_device) %>%
summarise(
mode           = first(mode),
model_name     = first(model_name),
resolution     = first(resolution),
fps_target     = first(fps_target),
codec          = first(codec),
tier           = first(tier),
# Average number of objects detected per frame
avg_detections = mean(detection_count, na.rm = TRUE),
.groups = "drop"
)
# Create a directory to store the detection plots
if (!dir.exists("plots_detections")) dir.create("plots_detections")
# 2. Triple-Nested Loop: FPS x Codec x Tier
# ---------------------------------------------------------
all_fps    <- sort(unique(df_detections$fps_target))
all_codecs <- unique(df_detections$codec)
all_tiers  <- sort(unique(df_detections$tier))
for (f in all_fps) {
for (c in all_codecs) {
for (t in all_tiers) {
# Filter for this specific scenario
plot_subset <- df_detections %>%
filter(fps_target == f, codec == c, tier == t)
if(nrow(plot_subset) == 0) next
# --- GENERATE DETECTION PLOT ---
p_det <- ggplot(plot_subset, aes(x = model_name, y = avg_detections, fill = mode)) +
# Compare Local vs Offload side-by-side
geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
# Add the 3 device points to check for consistency across RSUs
geom_jitter(position = position_dodge(width = 0.8), size = 1.2, alpha = 0.5) +
facet_wrap(~resolution) +
scale_fill_manual(values = c("Local" = "#2c3e50", "Offload" = "#8e44ad")) + # Purple for detections
labs(
title = paste("Avg Detections |", f, "FPS"),
subtitle = paste("Codec:", c, "| Quality Tier:", t),
x = "AI Model",
y = "Avg Detection Count per Frame",
fill = "Execution Mode"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# 3. SAVE RESULTS
# ---------------------------------------------------------
file_name <- paste0("plots_detections/det_fps", f, "_", c, "_", t, ".png")
ggsave(filename = file_name, plot = p_det, width = 10, height = 6)
message("Saved detection plot: ", file_name)
}
}
}
library(tidyverse)
library(vroom)
# 1. Load and Aggregation
# ---------------------------------------------------------
files <- list.files("logs/", pattern = "\\.csv$", full.names = TRUE)
df_detections <- vroom(files, id = "filepath", show_col_types = FALSE) %>%
mutate(
fps_target = as.numeric(str_extract(filepath, "\\d+(?=fps)")),
run_id = basename(filepath)
) %>%
group_by(run_id, source_device) %>%
summarise(
mode           = first(mode),
model_name     = first(model_name),
resolution     = first(resolution),
fps_target     = first(fps_target),
codec          = first(codec),
tier           = first(tier),
# Average number of objects detected per frame
avg_detections = mean(detection_count, na.rm = TRUE),
.groups = "drop"
)
# Create a directory to store the detection plots
if (!dir.exists("plots_detections")) dir.create("plots_detections")
# 2. Triple-Nested Loop: FPS x Codec x Tier
# ---------------------------------------------------------
all_fps    <- sort(unique(df_detections$fps_target))
all_codecs <- unique(df_detections$codec)
all_tiers  <- sort(unique(df_detections$tier))
for (f in all_fps) {
for (c in all_codecs) {
for (t in all_tiers) {
# Filter for this specific scenario
plot_subset <- df_detections %>%
filter(fps_target == f, codec == c, tier == t)
if(nrow(plot_subset) == 0) next
# --- GENERATE DETECTION PLOT ---
p_det <- ggplot(plot_subset, aes(x = model_name, y = avg_detections, fill = mode)) +
# Compare Local vs Offload side-by-side
geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), width = 0.7) +
# Add the 3 device points to check for consistency across RSUs
geom_jitter(position = position_dodge(width = 0.8), size = 1.2, alpha = 0.5) +
facet_wrap(~resolution) +
scale_fill_manual(values = c("Local" = "#2c3e50", "Offload" = "#8e44ad")) + # Purple for detections
labs(
title = paste("Avg Detections |", f, "FPS"),
subtitle = paste("Codec:", c, "| Quality Tier:", t),
x = "AI Model",
y = "Avg Detection Count per Frame",
fill = "Execution Mode"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# 3. SAVE RESULTS
# ---------------------------------------------------------
file_name <- paste0("plots_detections/det_fps", f, "_", c, "_", t, ".png")
ggsave(filename = file_name, plot = p_det, width = 10, height = 6)
message("Saved detection plot: ", file_name)
}
}
}